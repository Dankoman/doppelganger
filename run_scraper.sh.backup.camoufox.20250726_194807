#!/bin/bash

# Doppelganger Scraper - Enhanced Anti-Blocking Edition with Chrome Headless
# Hanteringsscript fÃ¶r Docker-baserad scraping

set -e

# FÃ¤rger fÃ¶r output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Banner
show_banner() {
    echo -e "${BLUE}"
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘                      ğŸ•·ï¸  Doppelganger Scraper v2.0                          â•‘"
    echo "â•‘              Enhanced Anti-Blocking + Chrome Headless Edition               â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo -e "${NC}"
}

# Kontrollera att Docker kÃ¶rs
check_docker() {
    if ! docker info >/dev/null 2>&1; then
        echo -e "${RED}âŒ Docker kÃ¶rs inte. Starta Docker fÃ¶rst.${NC}"
        exit 1
    fi
}

# Skapa nÃ¶dvÃ¤ndiga kataloger
ensure_directories() {
    mkdir -p images crawls logs httpcache
    chmod 755 images crawls logs httpcache
}

# URL-baserade scraper-funktioner
url_test() {
    show_banner
    echo -e "${YELLOW}ğŸ§ª Testar URL-baserad scraper med 5 profiler...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att URL-filen finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}âŒ profile_urls.txt saknas. Kopiera filen frÃ¥n HTML-extraktionen.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}ğŸ“Š Antal URL:er i fil: $(wc -l < profile_urls.txt)${NC}"
    
    # KÃ¶r test med URL-baserad spider
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -s CLOSESPIDER_ITEMCOUNT=5 \
        -s LOG_LEVEL=INFO
}

url_run() {
    show_banner
    echo -e "${YELLOW}ğŸš€ KÃ¶r URL-baserad scraper fÃ¶r alla profiler...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att URL-filen finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}âŒ profile_urls.txt saknas. Kopiera filen frÃ¥n HTML-extraktionen.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}ğŸ“Š Antal URL:er att scrapa: $(wc -l < profile_urls.txt)${NC}"
    echo -e "${YELLOW}âš ï¸  Detta kommer att scrapa ALLA profiler. FortsÃ¤tt? (y/N):${NC}"
    read -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        # KÃ¶r full URL-baserad scraping
        docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
            -s LOG_LEVEL=INFO
    else
        echo -e "${CYAN}â„¹ï¸  Scraping avbruten${NC}"
    fi
}

url_sample() {
    show_banner
    echo -e "${YELLOW}ğŸ“Š KÃ¶r URL-baserad scraper fÃ¶r 100 profiler...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att URL-filen finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}âŒ profile_urls.txt saknas. Kopiera filen frÃ¥n HTML-extraktionen.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}ğŸ“Š Antal URL:er i fil: $(wc -l < profile_urls.txt)${NC}"
    
    # KÃ¶r sample med URL-baserad spider
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -s CLOSESPIDER_ITEMCOUNT=100 \
        -s LOG_LEVEL=INFO
}

url_debug() {
    show_banner
    echo -e "${YELLOW}ğŸ› Debug URL-baserad scraper...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att URL-filen finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}âŒ profile_urls.txt saknas. Kopiera filen frÃ¥n HTML-extraktionen.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}ğŸ“Š FÃ¶rsta 5 URL:er:${NC}"
    head -5 profile_urls.txt
    echo ""
    
    # KÃ¶r debug med URL-baserad spider
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -L DEBUG \
        -s CLOSESPIDER_ITEMCOUNT=1 \
        -s DOWNLOAD_DELAY=10 \
        -s CONCURRENT_REQUESTS=1
}

# Chrome Headless-funktioner
chrome_test() {
    show_banner
    echo -e "${YELLOW}ğŸ§ª Testar Chrome headless-integration...${NC}"
    check_docker
    
    # Testa Chrome-anslutning frÃ¥n host
    echo -e "${CYAN}ğŸ” Testar Chrome-anslutning frÃ¥n host...${NC}"
    if command -v python3 &> /dev/null; then
        python3 test_chrome.py
    else
        echo -e "${YELLOW}âš ï¸  Python3 inte tillgÃ¤ngligt pÃ¥ host, hoppar Ã¶ver anslutningstest${NC}"
    fi
    
    echo -e "${CYAN}ğŸš€ Testar Chrome-scraper med 1 profil...${NC}"
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -s CLOSESPIDER_ITEMCOUNT=1 \
        -s LOG_LEVEL=INFO \
        -s CHROME_ENABLED=True \
        -s DOWNLOAD_DELAY=8 \
        -s CONCURRENT_REQUESTS=1
}

chrome_sample() {
    show_banner
    echo -e "${YELLOW}ğŸ“Š KÃ¶r Chrome-scraper fÃ¶r 10 profiler...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att profile_urls.txt finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}âŒ profile_urls.txt saknas. KÃ¶r fÃ¶rst url_test fÃ¶r att sÃ¤tta upp URL-listan.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}ğŸ“Š Antal URL:er: $(wc -l < profile_urls.txt)${NC}"
    
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -s CLOSESPIDER_ITEMCOUNT=10 \
        -s LOG_LEVEL=INFO \
        -s CHROME_ENABLED=True \
        -s DOWNLOAD_DELAY=8 \
        -s CONCURRENT_REQUESTS=1
}

chrome_run() {
    show_banner
    echo -e "${YELLOW}ğŸš€ KÃ¶r Chrome-scraper fÃ¶r alla profiler...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att profile_urls.txt finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}âŒ profile_urls.txt saknas. KÃ¶r fÃ¶rst url_test fÃ¶r att sÃ¤tta upp URL-listan.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}ğŸ“Š Antal URL:er att scrapa: $(wc -l < profile_urls.txt)${NC}"
    echo -e "${YELLOW}âš ï¸  Detta kommer att anvÃ¤nda Chrome headless fÃ¶r alla profiler. FortsÃ¤tt? (y/N):${NC}"
    read -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
            -s LOG_LEVEL=INFO \
            -s CHROME_ENABLED=True \
            -s DOWNLOAD_DELAY=8 \
            -s CONCURRENT_REQUESTS=1
    else
        echo -e "${CYAN}â„¹ï¸  Chrome-scraping avbruten${NC}"
    fi
}

chrome_debug() {
    show_banner
    echo -e "${YELLOW}ğŸ› Debug Chrome-scraper...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att profile_urls.txt finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}âŒ profile_urls.txt saknas. KÃ¶r fÃ¶rst url_test fÃ¶r att sÃ¤tta upp URL-listan.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}ğŸ“Š FÃ¶rsta 5 URL:er:${NC}"
    head -5 profile_urls.txt
    echo ""
    
    # KÃ¶r debug med Chrome
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -L DEBUG \
        -s CLOSESPIDER_ITEMCOUNT=1 \
        -s CHROME_ENABLED=True \
        -s DOWNLOAD_DELAY=10 \
        -s CONCURRENT_REQUESTS=1
}

# HjÃ¤lpfunktioner
show_help() {
    show_banner
    echo -e "${YELLOW}AnvÃ¤ndning:${NC} $0 [KOMMANDO]"
    echo ""
    echo -e "${CYAN}TillgÃ¤ngliga kommandon:${NC}"
    echo ""
    echo -e "  ${GREEN}build${NC}           Bygg Docker-imagen med alla fÃ¶rbÃ¤ttringar"
    echo -e "  ${GREEN}test${NC}            KÃ¶r test med begrÃ¤nsad scraping (5 items)"
    echo -e "  ${GREEN}run${NC}             KÃ¶r full scraping (interaktivt)"
    echo -e "  ${GREEN}run-detached${NC}    KÃ¶r full scraping i bakgrunden"
    echo -e "  ${GREEN}stop${NC}            Stoppa alla kÃ¶rande containers"
    echo -e "  ${GREEN}logs${NC}            Visa live loggar frÃ¥n scrapern"
    echo -e "  ${GREEN}shell${NC}           Ã–ppna bash-shell i container"
    echo -e "  ${GREEN}status${NC}          Visa status fÃ¶r containers"
    echo -e "  ${GREEN}clean${NC}           Rensa alla containers och volymer"
    echo -e "  ${GREEN}monitor${NC}        Starta monitoring-interface (port 8080)"
    echo -e "  ${GREEN}verify${NC}         Verifiera Brotli och andra fÃ¶rbÃ¤ttringar"
    echo -e "  ${GREEN}debug${NC}          KÃ¶r i debug-lÃ¤ge med detaljerade loggar"
    echo ""
    echo -e "${CYAN}URL-baserade kommandon (kringgÃ¥r blockerad huvudsida):${NC}"
    echo -e "  ${GREEN}url_test${NC}       Testa URL-baserad scraper (5 profiler)"
    echo -e "  ${GREEN}url_sample${NC}     Scrapa 100 profiler frÃ¥n URL-lista"
    echo -e "  ${GREEN}url_run${NC}        Scrapa alla profiler frÃ¥n URL-lista"
    echo -e "  ${GREEN}url_debug${NC}      Debug URL-baserad scraper"
    echo ""
    echo -e "${CYAN}Chrome Headless-kommandon (anvÃ¤nder chromedp/headless-shell):${NC}"
    echo -e "  ${GREEN}chrome_test${NC}     Testa Chrome headless-integration (1 profil)"
    echo -e "  ${GREEN}chrome_sample${NC}   Scrapa 10 profiler med Chrome headless"
    echo -e "  ${GREEN}chrome_run${NC}      Scrapa alla profiler med Chrome headless"
    echo -e "  ${GREEN}chrome_debug${NC}    Debug Chrome headless-scraper"
    echo ""
    echo -e "${YELLOW}Exempel:${NC}"
    echo -e "  $0 build && $0 test     ${PURPLE}# Bygg och testa${NC}"
    echo -e "  $0 url_test             ${PURPLE}# Testa URL-scraper${NC}"
    echo -e "  $0 chrome_test          ${PURPLE}# Testa Chrome headless${NC}"
    echo -e "  $0 chrome_sample        ${PURPLE}# Scrapa 10 profiler med Chrome${NC}"
    echo -e "  $0 logs                 ${PURPLE}# Ã–vervaka framsteg${NC}"
    echo ""
    echo -e "${CYAN}FÃ¶rbÃ¤ttringar i v2.0:${NC}"
    echo -e "  âœ… Brotli-komprimeringssstÃ¶d"
    echo -e "  âœ… 12 realistiska User Agents"
    echo -e "  âœ… Avancerade anti-blocking headers"
    echo -e "  âœ… Adaptiv fÃ¶rdrÃ¶jning vid 403-fel"
    echo -e "  âœ… Exponential backoff retry-logik"
    echo -e "  âœ… URL-baserad scraping (kringgÃ¥r blockering)"
    echo -e "  âœ… Chrome headless-integration (ultimat bot-bypass)"
    echo ""
}

# Huvudfunktioner
case "${1:-help}" in
    "build")
        show_banner
        echo -e "${YELLOW}ğŸ”¨ Bygger Docker-imagen med Enhanced Anti-Blocking + Chrome...${NC}"
        check_docker
        ensure_directories
        
        # Bygg imagen
        docker-compose build --no-cache
        
        echo -e "${GREEN}âœ… Docker-imagen byggd framgÃ¥ngsrikt!${NC}"
        echo -e "${CYAN}ğŸ’¡ Testa nu med: $0 test, $0 url_test eller $0 chrome_test${NC}"
        ;;
        
    "test")
        show_banner
        echo -e "${YELLOW}ğŸ§ª KÃ¶r test med begrÃ¤nsad scraping...${NC}"
        check_docker
        ensure_directories
        
        # KÃ¶r test med begrÃ¤nsning
        docker-compose run --rm doppelganger-scraper scrapy crawl mpb_all \
            -s CLOSESPIDER_ITEMCOUNT=5 \
            -s DOWNLOAD_DELAY=5 \
            -s CONCURRENT_REQUESTS=1 \
            -L INFO
        ;;
        
    "run")
        show_banner
        echo -e "${YELLOW}ğŸš€ KÃ¶r full scraping (interaktivt)...${NC}"
        check_docker
        ensure_directories
        
        # KÃ¶r full scraping
        docker-compose run --rm doppelganger-scraper scrapy crawl mpb_all
        ;;
        
    "run-detached")
        show_banner
        echo -e "${YELLOW}ğŸš€ KÃ¶r full scraping i bakgrunden...${NC}"
        check_docker
        ensure_directories
        
        # KÃ¶r i bakgrunden
        docker-compose up -d doppelganger-scraper
        echo -e "${GREEN}âœ… Scraper startad i bakgrunden${NC}"
        echo -e "${CYAN}ğŸ’¡ Visa loggar med: $0 logs${NC}"
        echo -e "${CYAN}ğŸ’¡ Stoppa med: $0 stop${NC}"
        ;;
        
    "stop")
        show_banner
        echo -e "${YELLOW}ğŸ›‘ Stoppar alla containers...${NC}"
        check_docker
        
        docker-compose down
        echo -e "${GREEN}âœ… Alla containers stoppade${NC}"
        ;;
        
    "logs")
        show_banner
        echo -e "${YELLOW}ğŸ“‹ Visar live loggar (Ctrl+C fÃ¶r att avsluta)...${NC}"
        check_docker
        
        docker-compose logs -f doppelganger-scraper
        ;;
        
    "shell")
        show_banner
        echo -e "${YELLOW}ğŸš Ã–ppnar bash-shell i container...${NC}"
        check_docker
        
        docker-compose run --rm doppelganger-scraper bash
        ;;
        
    "status")
        show_banner
        echo -e "${YELLOW}ğŸ“Š Container-status:${NC}"
        check_docker
        
        docker-compose ps
        echo ""
        echo -e "${YELLOW}ğŸ“ˆ Docker-statistik:${NC}"
        docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}"
        ;;
        
    "clean")
        show_banner
        echo -e "${YELLOW}ğŸ§¹ Rensar containers och volymer...${NC}"
        check_docker
        
        read -p "âš ï¸  Detta kommer att ta bort alla containers och data. FortsÃ¤tt? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            docker-compose down -v
            docker system prune -f
            echo -e "${GREEN}âœ… Rensning klar${NC}"
        else
            echo -e "${CYAN}â„¹ï¸  Rensning avbruten${NC}"
        fi
        ;;
        
    "monitor")
        show_banner
        echo -e "${YELLOW}ğŸ“Š Startar monitoring-interface...${NC}"
        check_docker
        ensure_directories
        
        # Starta monitoring
        docker-compose --profile monitoring up -d scraper-monitor
        echo -e "${GREEN}âœ… Monitoring startat pÃ¥ http://localhost:8080${NC}"
        echo -e "${CYAN}ğŸ’¡ Stoppa med: docker-compose --profile monitoring down${NC}"
        ;;
        
    "verify")
        show_banner
        echo -e "${YELLOW}ğŸ” Verifierar fÃ¶rbÃ¤ttringar...${NC}"
        check_docker
        
        echo -e "${CYAN}Testar Brotli-stÃ¶d:${NC}"
        docker-compose run --rm doppelganger-scraper python -c "
import brotli, brotlicffi
print('âœ… Brotli support: OK')
print(f'   brotli version: {brotli.__version__}')
print(f'   brotlicffi version: {brotlicffi.__version__}')
"
        
        echo -e "${CYAN}Testar komprimering:${NC}"
        docker-compose run --rm doppelganger-scraper python -c "
import brotli
data = b'Hello Enhanced Doppelganger!'
compressed = brotli.compress(data)
decompressed = brotli.decompress(compressed)
print(f'âœ… Compression test: {decompressed.decode()}')
print(f'   Original size: {len(data)} bytes')
print(f'   Compressed size: {len(compressed)} bytes')
print(f'   Compression ratio: {len(compressed)/len(data):.2%}')
"
        
        echo -e "${CYAN}Testar Scrapy-konfiguration:${NC}"
        docker-compose run --rm doppelganger-scraper python -c "
from scrapy.utils.project import get_project_settings
settings = get_project_settings()
print('âœ… Scrapy settings loaded')
print(f'   Download delay: {settings.get(\"DOWNLOAD_DELAY\")}s')
print(f'   Concurrent requests: {settings.get(\"CONCURRENT_REQUESTS\")}')
print(f'   Anti-blocking enabled: {settings.get(\"ANTIBLOCK_ENABLED\")}')
print(f'   Retry times: {settings.get(\"RETRY_TIMES\")}')
"
        
        echo -e "${GREEN}âœ… Alla verifieringar klara!${NC}"
        ;;
        
    "debug")
        show_banner
        echo -e "${YELLOW}ğŸ› KÃ¶r i debug-lÃ¤ge...${NC}"
        check_docker
        ensure_directories
        
        # KÃ¶r med debug och begrÃ¤nsning
        docker-compose run --rm doppelganger-scraper scrapy crawl mpb_all \
            -L DEBUG \
            -s CLOSESPIDER_ITEMCOUNT=1 \
            -s DOWNLOAD_DELAY=10 \
            -s CONCURRENT_REQUESTS=1
        ;;
        
    "url_test")
        url_test
        ;;
        
    "url_run")
        url_run
        ;;
        
    "url_sample")
        url_sample
        ;;
        
    "url_debug")
        url_debug
        ;;
        
    "chrome_test")
        chrome_test
        ;;
        
    "chrome_sample")
        chrome_sample
        ;;
        
    "chrome_run")
        chrome_run
        ;;
        
    "chrome_debug")
        chrome_debug
        ;;
        
    "help"|*)
        show_help
        ;;
    "camoufox_test")
        camoufox_test
        ;;
    "camoufox_sample")
        camoufox_sample
        ;;
    "camoufox_run")
        camoufox_run
        ;;
    "camoufox_debug")
        camoufox_debug
        ;;
    "camoufox_verify")
        camoufox_verify
        ;;
    "camoufox_stop")
        camoufox_stop
        ;;
    "camoufox_logs")
        camoufox_logs
        ;;
esac

# Camoufox-specifika funktioner
camoufox_test() {
    echo "ğŸ¦Š Testar Camoufox-integration..."
    docker-compose up -d camoufox-server
    sleep 10
    
    echo "ğŸ§ª Testar scraping med Camoufox (1 profil)..."
    docker-compose run --rm camoufox-scraper scrapy crawl mpb_from_urls \
        -s CLOSESPIDER_ITEMCOUNT=1 \
        -s CAMOUFOX_ENABLED=True \
        -s LOG_LEVEL=INFO \
        -s DOWNLOAD_DELAY=10
}

camoufox_sample() {
    echo "ğŸ¦Š Scrapar 10 profiler med Camoufox..."
    docker-compose up -d camoufox-server
    sleep 5
    
    docker-compose run --rm camoufox-scraper scrapy crawl mpb_from_urls \
        -s CLOSESPIDER_ITEMCOUNT=10 \
        -s CAMOUFOX_ENABLED=True \
        -s LOG_LEVEL=INFO \
        -s DOWNLOAD_DELAY=8
}

camoufox_run() {
    echo "ğŸ¦Š Scrapar alla profiler med Camoufox..."
    echo "âš ï¸ Detta kommer att ta mycket lÃ¥ng tid (6360 profiler)"
    read -p "Ã„r du sÃ¤ker? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        docker-compose up -d camoufox-server
        sleep 5
        
        docker-compose run --rm camoufox-scraper scrapy crawl mpb_from_urls \
            -s CAMOUFOX_ENABLED=True \
            -s LOG_LEVEL=INFO \
            -s DOWNLOAD_DELAY=8
    else
        echo "Avbrutet."
    fi
}

camoufox_debug() {
    echo "ğŸ¦Š Debug-lÃ¤ge fÃ¶r Camoufox..."
    docker-compose up -d camoufox-server
    sleep 5
    
    docker-compose run --rm camoufox-scraper scrapy crawl mpb_from_urls \
        -s CLOSESPIDER_ITEMCOUNT=1 \
        -s CAMOUFOX_ENABLED=True \
        -s LOG_LEVEL=DEBUG \
        -s DOWNLOAD_DELAY=15 \
        -s CAMOUFOX_CLOUDFLARE_WAIT=20
}

camoufox_verify() {
    echo "ğŸ” Verifierar Camoufox-installation..."
    docker-compose up -d camoufox-server
    sleep 10
    
    echo "ğŸ¦Š Kontrollerar Camoufox-server..."
    if docker-compose exec camoufox-server curl -s http://localhost:4444/wd/hub/status > /dev/null 2>&1; then
        echo "âœ… Camoufox-server fungerar"
    else
        echo "âŒ Camoufox-server svarar inte"
    fi
}

camoufox_stop() {
    echo "ğŸ›‘ Stoppar Camoufox-server..."
    docker-compose stop camoufox-server
}

camoufox_logs() {
    echo "ğŸ“‹ Visar Camoufox-server loggar..."
    docker-compose logs camoufox-server
}
