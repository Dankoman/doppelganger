#!/bin/bash

# Doppelganger Scraper - Enhanced Anti-Blocking Edition
# Hanteringsscript f√∂r Docker-baserad scraping

set -e

# F√§rger f√∂r output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Banner
show_banner() {
    echo -e "${BLUE}"
    echo "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
    echo "‚ïë                      üï∑Ô∏è  Doppelganger Scraper v2.0                          ‚ïë"
    echo "‚ïë                    Enhanced Anti-Blocking Edition                            ‚ïë"
    echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
    echo -e "${NC}"
}

# Kontrollera att Docker k√∂rs
check_docker() {
    if ! docker info >/dev/null 2>&1; then
        echo -e "${RED}‚ùå Docker k√∂rs inte. Starta Docker f√∂rst.${NC}"
        exit 1
    fi
}

# Skapa n√∂dv√§ndiga kataloger
ensure_directories() {
    mkdir -p images crawls logs httpcache
    chmod 755 images crawls logs httpcache
}

# URL-baserade scraper-funktioner
url_test() {
    show_banner
    echo -e "${YELLOW}üß™ Testar URL-baserad scraper med 5 profiler...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att URL-filen finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}‚ùå profile_urls.txt saknas. Kopiera filen fr√•n HTML-extraktionen.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}üìä Antal URL:er i fil: $(wc -l < profile_urls.txt)${NC}"
    
    # K√∂r test med URL-baserad spider
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -s CLOSESPIDER_ITEMCOUNT=5 \
        -s LOG_LEVEL=INFO
}

url_run() {
    show_banner
    echo -e "${YELLOW}üöÄ K√∂r URL-baserad scraper f√∂r alla profiler...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att URL-filen finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}‚ùå profile_urls.txt saknas. Kopiera filen fr√•n HTML-extraktionen.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}üìä Antal URL:er att scrapa: $(wc -l < profile_urls.txt)${NC}"
    echo -e "${YELLOW}‚ö†Ô∏è  Detta kommer att scrapa ALLA profiler. Forts√§tt? (y/N):${NC}"
    read -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        # K√∂r full URL-baserad scraping
        docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
            -s LOG_LEVEL=INFO
    else
        echo -e "${CYAN}‚ÑπÔ∏è  Scraping avbruten${NC}"
    fi
}

url_sample() {
    show_banner
    echo -e "${YELLOW}üìä K√∂r URL-baserad scraper f√∂r 100 profiler...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att URL-filen finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}‚ùå profile_urls.txt saknas. Kopiera filen fr√•n HTML-extraktionen.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}üìä Antal URL:er i fil: $(wc -l < profile_urls.txt)${NC}"
    
    # K√∂r sample med URL-baserad spider
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -s CLOSESPIDER_ITEMCOUNT=100 \
        -s LOG_LEVEL=INFO
}

url_debug() {
    show_banner
    echo -e "${YELLOW}üêõ Debug URL-baserad scraper...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att URL-filen finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}‚ùå profile_urls.txt saknas. Kopiera filen fr√•n HTML-extraktionen.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}üìä F√∂rsta 5 URL:er:${NC}"
    head -5 profile_urls.txt
    echo ""
    
    # K√∂r debug med URL-baserad spider
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -L DEBUG \
        -s CLOSESPIDER_ITEMCOUNT=1 \
        -s DOWNLOAD_DELAY=10 \
        -s CONCURRENT_REQUESTS=1
}

# Hj√§lpfunktioner
show_help() {
    show_banner
    echo -e "${YELLOW}Anv√§ndning:${NC} $0 [KOMMANDO]"
    echo ""
    echo -e "${CYAN}Tillg√§ngliga kommandon:${NC}"
    echo ""
    echo -e "  ${GREEN}build${NC}           Bygg Docker-imagen med alla f√∂rb√§ttringar"
    echo -e "  ${GREEN}test${NC}            K√∂r test med begr√§nsad scraping (5 items)"
    echo -e "  ${GREEN}run${NC}             K√∂r full scraping (interaktivt)"
    echo -e "  ${GREEN}run-detached${NC}    K√∂r full scraping i bakgrunden"
    echo -e "  ${GREEN}stop${NC}            Stoppa alla k√∂rande containers"
    echo -e "  ${GREEN}logs${NC}            Visa live loggar fr√•n scrapern"
    echo -e "  ${GREEN}shell${NC}           √ñppna bash-shell i container"
    echo -e "  ${GREEN}status${NC}          Visa status f√∂r containers"
    echo -e "  ${GREEN}clean${NC}           Rensa alla containers och volymer"
    echo -e "  ${GREEN}monitor${NC}        Starta monitoring-interface (port 8080)"
    echo -e "  ${GREEN}verify${NC}         Verifiera Brotli och andra f√∂rb√§ttringar"
    echo -e "  ${GREEN}debug${NC}          K√∂r i debug-l√§ge med detaljerade loggar"
    echo ""
    echo -e "${CYAN}URL-baserade kommandon (kringg√•r blockerad huvudsida):${NC}"
    echo -e "  ${GREEN}url_test${NC}       Testa URL-baserad scraper (5 profiler)"
    echo -e "  ${GREEN}url_sample${NC}     Scrapa 100 profiler fr√•n URL-lista"
    echo -e "  ${GREEN}url_run${NC}        Scrapa alla profiler fr√•n URL-lista"
    echo -e "  ${GREEN}url_debug${NC}      Debug URL-baserad scraper"
    echo ""
    echo -e "${CYAN}Chrome Headless-kommandon (anv√§nder chromedp/headless-shell):${NC}"
    echo -e "  ${GREEN}chrome_test${NC}     Testa Chrome headless-integration (1 profil)"
    echo -e "  ${GREEN}chrome_sample${NC}   Scrapa 10 profiler med Chrome headless"
    echo -e "  ${GREEN}chrome_run${NC}      Scrapa alla profiler med Chrome headless"
    echo -e "  ${GREEN}chrome_debug${NC}    Debug Chrome headless-scraper"
    echo ""
    echo -e "${YELLOW}Exempel:${NC}"
    echo -e "  $0 build && $0 test     ${PURPLE}# Bygg och testa${NC}"
    echo -e "  $0 url_test             ${PURPLE}# Testa URL-scraper${NC}"
    echo -e "  $0 chrome_test          ${PURPLE}# Testa Chrome headless${NC}"
    echo -e "  $0 chrome_sample        ${PURPLE}# Scrapa 10 profiler med Chrome${NC}"
    echo -e "  $0 logs                 ${PURPLE}# √ñvervaka framsteg${NC}"
    echo ""
    echo -e "${CYAN}F√∂rb√§ttringar i v2.0:${NC}"
    echo -e "  ‚úÖ Brotli-komprimeringssst√∂d"
    echo -e "  ‚úÖ 12 realistiska User Agents"
    echo -e "  ‚úÖ Avancerade anti-blocking headers"
    echo -e "  ‚úÖ Adaptiv f√∂rdr√∂jning vid 403-fel"
    echo -e "  ‚úÖ Exponential backoff retry-logik"
    echo -e "  ‚úÖ URL-baserad scraping (kringg√•r blockering)"
    echo -e "  ‚úÖ Chrome headless-integration (ultimat bot-bypass)"
    echo ""
}

# Huvudfunktioner
case "${1:-help}" in
    "build")
        show_banner
        echo -e "${YELLOW}üî® Bygger Docker-imagen med Enhanced Anti-Blocking...${NC}"
        check_docker
        ensure_directories
        
        # Bygg imagen
        docker-compose build --no-cache
        
        echo -e "${GREEN}‚úÖ Docker-imagen byggd framg√•ngsrikt!${NC}"
        echo -e "${CYAN}üí° Testa nu med: $0 test eller $0 url_test${NC}"
        ;;
        
    "test")
        show_banner
        echo -e "${YELLOW}üß™ K√∂r test med begr√§nsad scraping...${NC}"
        check_docker
        ensure_directories
        
        # K√∂r test med begr√§nsning
        docker-compose run --rm doppelganger-scraper scrapy crawl mpb_all \
            -s CLOSESPIDER_ITEMCOUNT=5 \
            -s DOWNLOAD_DELAY=5 \
            -s CONCURRENT_REQUESTS=1 \
            -L INFO
        ;;
        
    "run")
        show_banner
        echo -e "${YELLOW}üöÄ K√∂r full scraping (interaktivt)...${NC}"
        check_docker
        ensure_directories
        
        # K√∂r full scraping
        docker-compose run --rm doppelganger-scraper scrapy crawl mpb_all
        ;;
        
    "run-detached")
        show_banner
        echo -e "${YELLOW}üöÄ K√∂r full scraping i bakgrunden...${NC}"
        check_docker
        ensure_directories
        
        # K√∂r i bakgrunden
        docker-compose up -d doppelganger-scraper
        echo -e "${GREEN}‚úÖ Scraper startad i bakgrunden${NC}"
        echo -e "${CYAN}üí° Visa loggar med: $0 logs${NC}"
        echo -e "${CYAN}üí° Stoppa med: $0 stop${NC}"
        ;;
        
    "stop")
        show_banner
        echo -e "${YELLOW}üõë Stoppar alla containers...${NC}"
        check_docker
        
        docker-compose down
        echo -e "${GREEN}‚úÖ Alla containers stoppade${NC}"
        ;;
        
    "logs")
        show_banner
        echo -e "${YELLOW}üìã Visar live loggar (Ctrl+C f√∂r att avsluta)...${NC}"
        check_docker
        
        docker-compose logs -f doppelganger-scraper
        ;;
        
    "shell")
        show_banner
        echo -e "${YELLOW}üêö √ñppnar bash-shell i container...${NC}"
        check_docker
        
        docker-compose run --rm doppelganger-scraper bash
        ;;
        
    "status")
        show_banner
        echo -e "${YELLOW}üìä Container-status:${NC}"
        check_docker
        
        docker-compose ps
        echo ""
        echo -e "${YELLOW}üìà Docker-statistik:${NC}"
        docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}"
        ;;
        
    "clean")
        show_banner
        echo -e "${YELLOW}üßπ Rensar containers och volymer...${NC}"
        check_docker
        
        read -p "‚ö†Ô∏è  Detta kommer att ta bort alla containers och data. Forts√§tt? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            docker-compose down -v
            docker system prune -f
            echo -e "${GREEN}‚úÖ Rensning klar${NC}"
        else
            echo -e "${CYAN}‚ÑπÔ∏è  Rensning avbruten${NC}"
        fi
        ;;
        
    "monitor")
        show_banner
        echo -e "${YELLOW}üìä Startar monitoring-interface...${NC}"
        check_docker
        ensure_directories
        
        # Starta monitoring
        docker-compose --profile monitoring up -d scraper-monitor
        echo -e "${GREEN}‚úÖ Monitoring startat p√• http://localhost:8080${NC}"
        echo -e "${CYAN}üí° Stoppa med: docker-compose --profile monitoring down${NC}"
        ;;
        
    "verify")
        show_banner
        echo -e "${YELLOW}üîç Verifierar f√∂rb√§ttringar...${NC}"
        check_docker
        
        echo -e "${CYAN}Testar Brotli-st√∂d:${NC}"
        docker-compose run --rm doppelganger-scraper python -c "
import brotli, brotlicffi
print('‚úÖ Brotli support: OK')
print(f'   brotli version: {brotli.__version__}')
print(f'   brotlicffi version: {brotlicffi.__version__}')
"
        
        echo -e "${CYAN}Testar komprimering:${NC}"
        docker-compose run --rm doppelganger-scraper python -c "
import brotli
data = b'Hello Enhanced Doppelganger!'
compressed = brotli.compress(data)
decompressed = brotli.decompress(compressed)
print(f'‚úÖ Compression test: {decompressed.decode()}')
print(f'   Original size: {len(data)} bytes')
print(f'   Compressed size: {len(compressed)} bytes')
print(f'   Compression ratio: {len(compressed)/len(data):.2%}')
"
        
        echo -e "${CYAN}Testar Scrapy-konfiguration:${NC}"
        docker-compose run --rm doppelganger-scraper python -c "
from scrapy.utils.project import get_project_settings
settings = get_project_settings()
print('‚úÖ Scrapy settings loaded')
print(f'   Download delay: {settings.get(\"DOWNLOAD_DELAY\")}s')
print(f'   Concurrent requests: {settings.get(\"CONCURRENT_REQUESTS\")}')
print(f'   Anti-blocking enabled: {settings.get(\"ANTIBLOCK_ENABLED\")}')
print(f'   Retry times: {settings.get(\"RETRY_TIMES\")}')
"
        
        echo -e "${GREEN}‚úÖ Alla verifieringar klara!${NC}"
        ;;
        
    "debug")
        show_banner
        echo -e "${YELLOW}üêõ K√∂r i debug-l√§ge...${NC}"
        check_docker
        ensure_directories
        
        # K√∂r med debug och begr√§nsning
        docker-compose run --rm doppelganger-scraper scrapy crawl mpb_all \
            -L DEBUG \
            -s CLOSESPIDER_ITEMCOUNT=1 \
            -s DOWNLOAD_DELAY=10 \
            -s CONCURRENT_REQUESTS=1
        ;;
        
    "url_test")
        url_test
        ;;
        
    "url_run")
        url_run
        ;;
        
    "url_sample")
        url_sample
        ;;
        
    "url_debug")
        url_debug
        ;;
        
    "chrome_test")
        chrome_test
        ;;
        
    "chrome_sample")
        chrome_sample
        ;;
        
    "chrome_run")
        chrome_run
        ;;
        
    "chrome_debug")
        chrome_debug
        ;;
        
    "help"|*)
        show_help
        ;;
esac

# Chrome Headless-kommandon
chrome_test() {
    show_banner
    echo -e "${YELLOW}üß™ Testar Chrome headless-integration...${NC}"
    check_docker
    
    # Testa Chrome-anslutning fr√•n host
    echo -e "${CYAN}üîç Testar Chrome-anslutning fr√•n host...${NC}"
    if command -v python3 &> /dev/null; then
        python3 test_chrome.py
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Python3 inte tillg√§ngligt p√• host, hoppar √∂ver anslutningstest${NC}"
    fi
    
    echo -e "${CYAN}üöÄ Testar Chrome-scraper med 1 profil...${NC}"
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -s CLOSESPIDER_ITEMCOUNT=1 \
        -s LOG_LEVEL=INFO \
        -s CHROME_ENABLED=True \
        -s DOWNLOAD_DELAY=8 \
        -s CONCURRENT_REQUESTS=1
}

chrome_sample() {
    show_banner
    echo -e "${YELLOW}üìä K√∂r Chrome-scraper f√∂r 10 profiler...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att profile_urls.txt finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}‚ùå profile_urls.txt saknas. K√∂r f√∂rst url_test f√∂r att s√§tta upp URL-listan.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}üìä Antal URL:er: $(wc -l < profile_urls.txt)${NC}"
    
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -s CLOSESPIDER_ITEMCOUNT=10 \
        -s LOG_LEVEL=INFO \
        -s CHROME_ENABLED=True \
        -s DOWNLOAD_DELAY=8 \
        -s CONCURRENT_REQUESTS=1
}

chrome_run() {
    show_banner
    echo -e "${YELLOW}üöÄ K√∂r Chrome-scraper f√∂r alla profiler...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att profile_urls.txt finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}‚ùå profile_urls.txt saknas. K√∂r f√∂rst url_test f√∂r att s√§tta upp URL-listan.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}üìä Antal URL:er att scrapa: $(wc -l < profile_urls.txt)${NC}"
    echo -e "${YELLOW}‚ö†Ô∏è  Detta kommer att anv√§nda Chrome headless f√∂r alla profiler. Forts√§tt? (y/N):${NC}"
    read -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
            -s LOG_LEVEL=INFO \
            -s CHROME_ENABLED=True \
            -s DOWNLOAD_DELAY=8 \
            -s CONCURRENT_REQUESTS=1
    else
        echo -e "${CYAN}‚ÑπÔ∏è  Chrome-scraping avbruten${NC}"
    fi
}

chrome_debug() {
    show_banner
    echo -e "${YELLOW}üêõ Debug Chrome-scraper...${NC}"
    check_docker
    ensure_directories
    
    # Kontrollera att profile_urls.txt finns
    if [ ! -f "profile_urls.txt" ]; then
        echo -e "${RED}‚ùå profile_urls.txt saknas. K√∂r f√∂rst url_test f√∂r att s√§tta upp URL-listan.${NC}"
        exit 1
    fi
    
    echo -e "${CYAN}üìä F√∂rsta 5 URL:er:${NC}"
    head -5 profile_urls.txt
    echo ""
    
    # K√∂r debug med Chrome
    docker-compose run --rm doppelganger-scraper scrapy crawl mpb_from_urls \
        -L DEBUG \
        -s CLOSESPIDER_ITEMCOUNT=1 \
        -s CHROME_ENABLED=True \
        -s DOWNLOAD_DELAY=10 \
        -s CONCURRENT_REQUESTS=1
}

